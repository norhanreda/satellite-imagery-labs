{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Students Information\n","\n","Please enter the names and IDs of the two students below:\n","\n","1. **Name**: [Enter Student 1 Name Here]  \n","   **ID**: `9XXXXXX` \n","\n","2. **Name**: [Enter Student 2 Name Here]  \n","   **ID**: `9XXXXXX` \n"]},{"cell_type":"markdown","metadata":{},"source":["## Students Instructions\n","\n","This is your first graded lab assignment, as you put the work you have studied in the lectures in action, please take this opportunity to enhance your understanding of the concepts and hone your skills. As you work on your assignment, please keep the following instructions in mind:\n","\n","- Clearly state your personal information where indicated.\n","- Be ready with your work before the time of the next discussion slot in the schedule.\n","- Plagiarism will be met with penalties, refrain from copying any answers to make the most out of the assignment. If any signs of plagiarism are detected, actions will be taken.\n","- It is acceptable to share the workload of the assignment bearing the discussion in mind.\n","- Feel free to [reach out](mailto:cmpsy27@gmail.com) if there were any ambiguities or post on the classroom."]},{"cell_type":"markdown","metadata":{},"source":["## Submission Instructions\n","\n","To ensure a smooth evaluation process, please follow these steps for submitting your work:\n","\n","1. **Prepare Your Submission:** Alongside your main notebook, include any additional files that are necessary for running the notebook successfully. This might include data files, images, or supplementary scripts.\n","\n","2. **Rename Your Files:** Before submission, please rename your notebook to reflect the IDs of the two students working on this project. The format should be `ID1_ID2`, where `ID1` and `ID2` are the student IDs. For example, if the student IDs are `9123456` and `9876543`, then your notebook should be named `9123456_9876543.ipynb`.\n","\n","3. **Check for Completeness:** Ensure that all required tasks are completed and that the notebook runs from start to finish without errors. This step is crucial for a smooth evaluation.\n","\n","4. **Submit Your Work:** Once everything is in order, submit your notebook and any additional files via the designated submission link on Google Classroom **(code: 2yj6e24)**. Make sure you meet the submission deadline to avoid any late penalties.\n","5. Please, note that the same student should submit the assignments for the pair throughout the semester.\n","\n","By following these instructions carefully, you help us in evaluating your work efficiently and fairly **and any failure to adhere to these guidelines can affect your grades**. If you encounter any difficulties or have questions about the submission process, please reach out as soon as possible.\n","\n","We look forward to seeing your completed projects and wish you the best of luck!\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","## Installation Instructions\n","\n","In this lab assignment, we require additional Python libraries for scientific mathematics, particularly in the context of machine learning (ML) and satellite image analysis. To fulfill these requirements, we need to install Scikit-learn and Scikit-image. \n","1. Install Scikit-learn  \n","Scikit-learn (Sklearn) is a powerful Python library for ML tasks, offering various algorithms for classification, regression, clustering, and model evaluation. It is extensively used for analyzing satellite imagery, enabling tasks such as land cover classification and environmental parameter prediction. On the other hand, Scikit-image (Skimage) provides comprehensive tools for image processing and computer vision, facilitating tasks such as image preprocessing, feature extraction, and segmentation. These libraries are essential for extracting valuable insights from satellite images and conducting advanced analysis in scientific computing and research domains.\n","```bash\n","pip install scikit-learn scikit-image\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["> **Note:** You are allowed to install any other necessary libraries you deem useful for solving the lab. Please ensure that any additional libraries are compatible with the project requirements and are properly documented in your submission.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Maximum Likelihood Estimator (MLE) Classifier\n","The Maximum Likelihood Estimator (MLE) is a fundamental statistical approach used to infer the parameters of a given distribution that are most likely to result in the observed data. In the context of image classification, MLE helps to quantify the probability of observing the data within each predefined class based on their distinct statistical properties. This method is highly effective for classifying images into categories by comparing the likelihoods of the data under different model parameters, enabling the most probable class assignment.\n","\n","1. **Calculate Class Priors**: Estimate the probability of each class based on the training dataset. This is expressed as:\n","   $$\n","   P(C_k) = \\frac{N_k}{N}\n","   $$\n","   where \\(N_k\\) is the number of samples of class \\(k\\) and \\(N\\) is the total number of samples.\n","\n","2. **Estimate Class-specific Parameters**: For each class, estimate parameters such as the mean \\(\\mu_k\\) and covariance \\(\\Sigma_k\\) of features that describe the distribution of the data:\n","   $$\n","   \\mu_k = \\frac{1}{N_k} \\sum_{x \\in C_k} x\n","   $$\n","   $$\n","   \\Sigma_k = \\frac{1}{N_k} \\sum_{x \\in C_k} (x - \\mu_k)(x - \\mu_k)^T\n","   $$\n","\n","3. **Compute Likelihoods**: For a given test instance \\(x\\), compute the likelihood of that instance belonging to each class using the estimated parameters:\n","   $$\n","   p(x | C_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right)\n","   $$\n","\n","4. **Classify Based on Maximum Likelihood**: Assign the class label to each test instance based on the highest likelihood, which can be calculated as:\n","   $$\n","   \\hat{y} = \\arg\\max_{k} P(C_k) \\cdot p(x | C_k)\n","   $$\n","\n","The Naive Bayes classifier is perhaps the most well-known application of the Maximum Likelihood Estimator principle in classification tasks. It assumes that the features in each class are independent, simplifying the computation of likelihoods. While Naive Bayes is popular for its simplicity and efficiency, it is not the only technique that leverages the MLE approach. Other classical alternatives include Logistic Regression, which applies MLE to estimate the parameters that best predict categorical outcomes, and Gaussian Mixture Models, which use MLE to estimate the parameters of multiple Gaussian distributions within the data. Students are encouraged to explore these models to gain a deeper understanding of statistical estimation techniques.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Req- Image Classification for EuroSATallBands\n","Image classification is a key challenge in satellite imaging and remote sensing. As discussed in the lecture, this task is typically conducted on a pixel-wise basis because a single image can contain multiple textural elements of different celestial features. However, for this specific assignment, we will focus on identifying the dominant phenomena in the image as the basis for classification.\n","\n","- **Load the Images**: Load the images of the EuroSAT dataset that belong to the **residential**, **river**, and **forest** classes.\n","\n","- **Split the Dataset**: Split the dataset such that 10% of each class is used as testing data, and the remainder is used for training your classifier. Use the indices provided by `np.random.choice` with seed set to `27`. **Code is provided do not change it**.\n","\n","- **Feature Extraction**: Extract suitable features from the images that you think might be relevant in distinguishing each class from the others. Keep in mind the curse of dimensionality when selecting features.\n","\n","- **Implement a Maximum Likelihood Estimator (MLE)**: Implement a Maximum Likelihood Estimator (MLE) based on your training data. \n","- **Report Accuracy and Average F1 Score**: After testing your classifier on the test set, report the **Accuracy** and **Average F1 Score** of your model.\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# Add your libraries here\n","import numpy as np\n","from skimage import io\n","import os\n","import numpy as np\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# DO NOT CHANGE THIS CELL\n","## Training set indices.\n","np.random.seed(27)  # Set random seed for reproducibility\n","\n","# Randomly select indices for the test sets for each class\n","residential_test_indices = np.random.choice(np.arange(3000), size=300, replace=False)\n","forest_test_indices = np.random.choice(np.arange(3000), size=300, replace=False)\n","river_test_indices = np.random.choice(np.arange(2500), size=250, replace=False)\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1748 1976 2825 1611  404  502  843 2977 1657  168 2972 2989   88  110\n"," 1853 1121 2640 1075 2398 1705  809 1231  466 2068 1545  713 1780  493\n"," 1884 2118 2020 1097 1601 1610 1456 1394 2472  602 1617 2809  706 2641\n","  592  108  351 1752  401 1233 2269  393 1720   64 2883 2504  828 2837\n","  293   75 2584 1994  289 1903  615 1005 1700 2770  389  472 2686   43\n"," 1528  232  500 2819 1852 1531 2084 1498 1722 2387  877  601 1640  583\n"," 2209 1536 2939 1411 1057 2391 2918   45  253 1189 2488 2991  366 2214\n"," 1638  710  529  211 2028  418 2895 2548 2364  195  552 1192 1403 1653\n","  468 2956 1949 2901 2854 2047  632 1261 2790  188 1726 2151 2396 1204\n"," 1494 2145  489 2846 2355 2104 1627 2228 2194 1733  691 2434  823 2799\n"," 2291 1584 2802  819  674   95  985 1093  642 1644 2539 1408 2656 2751\n"," 2927 2574  643  191 1680 2486 2630 1489 1918 1119 2162  955 1905 1191\n"," 1571  785 1407  395 2832  203 2362  438 2468 2378 2661 1220 1459 1254\n","  735 1225 1386  378 1018 1523 2739  867  423  202 1755 1108 2764  420\n"," 2881  692 1522 1565  801 2575 1455 2353 1045 1092  975 2994 1899  881\n"," 1217  539 1000 1335  414  346 1420  945 2635 1027 2747 2926 1074 2728\n"," 1123 1964  120 1946 1928 1184 2445 2125 2657 2241 1890 2717  118  535\n"," 1835 1824 2920 2655 1398 2368 1900 2324 1769 2829 1271  904  286 1651\n","  113 2966 2008 2723   70 2556 2611 2522 2601  284 1339 2496  917 1316\n"," 2891  918 1141  462 1865 1107  334 2771 1066 2233 2122 1718 2334  786\n"," 1014 1739  683  324  359  180 2541 2750  182    5   58  413  538  220\n","  426 1926 1570 2512 2577 2543]\n"]}],"source":["# Follow the steps \n","print(residential_test_indices )\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load the Images"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[1482 1088 1051 ... 2077 1236 2429]\n","  [1482 1088 1051 ... 2077 1236 2429]\n","  [1480 1145 1018 ... 1801 1084 2184]\n","  ...\n","  [1392 1138 1068 ... 1676 1163 2475]\n","  [1394 1240 1080 ... 1694 1198 2476]\n","  [1397 1262 1295 ... 1698 1192 2595]]\n","\n"," [[1370 1079  956 ... 1641 1132 2266]\n","  [1370 1079  956 ... 1641 1132 2266]\n","  [1376 1013  901 ... 1630 1147 2203]\n","  ...\n","  [1411 1073  971 ... 1729 1124 2435]\n","  [1410 1108  880 ... 1704 1111 2343]\n","  [1410 1073  881 ... 1739 1166 2263]]\n","\n"," [[1393 1235 1168 ... 2649 1682 3508]\n","  [1393 1235 1168 ... 2649 1682 3508]\n","  [1394 1313 1371 ... 2731 1839 3342]\n","  ...\n","  [1355 1163 1151 ... 2532 1445 3890]\n","  [1351 1204 1190 ... 2502 1363 3966]\n","  [1349 1097 1076 ... 2445 1274 3954]]\n","\n"," ...\n","\n"," [[1601 1519 1386 ... 2326 1844 2167]\n","  [1601 1519 1386 ... 2326 1844 2167]\n","  [1591 1601 1568 ... 2472 1966 2265]\n","  ...\n","  [1646 1520 1493 ... 2630 2029 2693]\n","  [1650 1546 1442 ... 2599 2012 2599]\n","  [1652 1412 1250 ... 2548 1966 2530]]\n","\n"," [[1162  848  748 ... 1531  845 3809]\n","  [1162  848  748 ... 1531  845 3809]\n","  [1157  845  770 ... 1525  832 3814]\n","  ...\n","  [1184  821  693 ... 1799 1060 2809]\n","  [1176  865  801 ... 1848 1010 3106]\n","  [1168  896  886 ... 1905 1001 3306]]\n","\n"," [[1580 1571 1281 ... 2270 1860 2573]\n","  [1580 1571 1281 ... 2270 1860 2573]\n","  [1590 2731 3262 ... 2449 2034 2652]\n","  ...\n","  [1485 1291 1344 ... 2372 1672 2535]\n","  [1493 1154 1065 ... 2241 1547 2417]\n","  [1501 1196 1134 ... 2060 1427 2267]]]\n"]}],"source":["dataset_folder = \"D:/Astudy/fourth year/second term/satellite imagery/dataset/EuroSATallBands/EuroSATallBands/ds/images/remote_sensing/otherDatasets/sentinel_2/tif\" \n","classes = [\"residential\", \"river\", \"forest\"]\n","images_by_class = {}\n","\n","for class_name in classes:\n","  \n","    class_folder = os.path.join(dataset_folder, class_name)\n","    if os.path.exists(class_folder):\n","        \n","        images_by_class[class_name] = []\n","        for image_file in os.listdir(class_folder):\n","            \n","            image_path = os.path.join(class_folder, image_file)\n","            image = io.imread(image_path)\n","            image = image.reshape(image.shape[0] * image.shape[1], image.shape[2])\n","            images_by_class[class_name].append(image)\n","\n","\n","\n","residential_images = np.array(images_by_class[\"residential\"])\n","\n","river_images = np.array(images_by_class[\"river\"])\n","\n","forest_images = np.array(images_by_class[\"forest\"])\n","\n","\n","print(residential_images)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" # Split the Dataset"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3000\n","2500\n","3000\n","(2700, 4096, 13)\n","(2250, 4096, 13)\n","(2700, 4096, 13)\n"]}],"source":["print(len(residential_images))\n","print(len(river_images))\n","print(len(forest_images))\n","\n","\n","\n","# print(residential_test_indices)\n","test_residual = residential_images[residential_test_indices]\n","test_river = river_images[river_test_indices]\n","test_forest =  forest_images[forest_test_indices]\n","\n","# print(test_residual)\n","# Generate the training indices by excluding the test indices\n","residential_training_indices = np.setdiff1d(np.arange(3000), residential_test_indices)\n","forest_training_indices = np.setdiff1d(np.arange(3000), forest_test_indices)\n","river_training_indices = np.setdiff1d(np.arange(2500), river_test_indices)\n","\n","train_residual = residential_images[residential_training_indices]\n","train_river = river_images[river_training_indices]\n","train_forest =forest_images[forest_training_indices]\n","\n","print(train_residual.shape)\n","print(train_river.shape)\n","print(train_forest.shape)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2700\n","(2250, 4096, 13)\n","(2700, 4096, 13)\n"]}],"source":["print(train_residual.shape[0])\n","print(train_river.shape)\n","print(train_forest.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["# Implement a Maximum Likelihood Estimator (MLE)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["p_residual:  0.35294117647058826\n","p_river:  0.29411764705882354\n","p_forest:  0.35294117647058826\n","sum prob:  1.0\n"]}],"source":["def get_Class_Priors(train_residual,train_forest,train_river):\n","  total_len = train_forest.shape[0] + train_residual.shape[0] + train_river.shape[0]\n","  p_residual =  train_residual.shape[0]/total_len\n","  p_river =  train_river.shape[0]/total_len\n","  p_forest = train_forest.shape[0]/total_len\n","\n","  return p_residual,p_river,p_forest\n","\n","p_residual,p_river,p_forest = get_Class_Priors(train_residual,train_forest,train_river)\n","print(\"p_residual: \",p_residual)\n","print(\"p_river: \",p_river)\n","print(\"p_forest: \",p_forest)\n","\n","print(\"sum prob: \",p_forest+p_residual+p_river)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def Estimate_Class_specific_Parameters(class_array):\n","    images_mean = np.zeros(class_array[0].shape[1])\n","    for i in range(class_array.shape[0]):\n","        image = class_array[i]\n","        image_mean =  np.mean(image, axis=0)\n","       \n","        images_mean += image_mean\n","    # print(images_mean)\n","    class_mean = images_mean/ class_array.shape[0]\n","    # new_image = []\n","    new_image = np.zeros((13, 13))\n","    for i in range (class_array.shape[0]):\n","        x_minus_mean = []\n","        image = class_array[i]\n","        for pixel in image: \n","            new_val = np.array(pixel-class_mean)\n","            new_val_mul = np.outer(new_val ,new_val) \n","            # print(new_val_mul)\n","            # print(new_val_mul.shape )\n","            # break\n","            # new_image.append( new_val_mul )\n","            new_image += new_val_mul\n","        # new_image.append(x_minus_mean)    \n","        # print(\"x_minus mean\",x_minus_mean[0])\n","        # print(\"x\",pixel)\n","        # break\n","    # sum = np.sum(new_image,axis=1)  \n","    class_covariance =   new_image / (class_array.shape[1]*class_array.shape[0])\n","    # print(new_image)\n","    # print(class_mean)\n","    # print(class_array.shape[0])\n","    # print(class_array[0].shape[1])\n","    return class_mean  ,  class_covariance     \n","\n","residual_mean ,residula_covariance= Estimate_Class_specific_Parameters(train_residual)\n","river_mean,river_covariance = Estimate_Class_specific_Parameters(train_river)\n","forest_mean ,forest_covariance = Estimate_Class_specific_Parameters(train_forest)\n","# print(\"forest_mean\",forest_mean)\n","# print(\"forest_covariance\",forest_covariance)\n","# print(\"river_mean\",river_mean)\n","# print(\"residual_mean\",residual_mean)\n","    "]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(13,)\n","3.703474083143855e-32\n"]}],"source":["def Compute_Likelihoods(x,mean,covariance):\n","   \n","    dim = len(mean)\n","\n","    # Calculate the normalization constant\n","    norm_const = 1 / ((2 * np.pi) ** (dim / 2) * np.linalg.det(covariance) ** 0.5)\n","\n","    # Calculate the exponent term\n","    exponent = -0.5 * np.dot(np.dot((x - mean).T, np.linalg.inv(covariance)), (x - mean))\n","\n","    # Calculate the probability density\n","    prob_density = norm_const * np.exp(exponent)\n","    return prob_density\n","print(test_forest[0][0].shape)\n","print(Compute_Likelihoods(test_forest[0][0],forest_mean,forest_covariance))\n","        "]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["forest\n"]}],"source":["def Classify_Based_on_Maximum_Likelihood(x_image_test,mean_class1,covariance_class1,mean_class2,covariance_class2,mean_class3,covariance_class3,p_residual,p_river,p_forest):\n","    class1_liklihood = 1\n","    class2_liklihood = 1\n","    class3_liklihood = 1\n","    for pixel in x_image_test:\n","       class1_liklihood *=  Compute_Likelihoods(pixel,mean_class1,covariance_class1)*p_residual\n","       class2_liklihood *=  Compute_Likelihoods(pixel,mean_class2,covariance_class2)*p_river\n","       class3_liklihood *=  Compute_Likelihoods(pixel,mean_class3,covariance_class3)*p_forest\n","       \n","       if(class1_liklihood==0 or class2_liklihood ==0 or class3_liklihood==0):\n","          break  \n","       max_class = max(class1_liklihood, class2_liklihood, class3_liklihood)\n","    max_class = max(class1_liklihood, class2_liklihood, class3_liklihood)   \n","    # print(class1_liklihood,class2_liklihood,class3_liklihood,max_class)\n","   \n","    result_class = None\n","    if(max_class == class1_liklihood):\n","        result_class = \"residual\"\n","    elif (max_class == class2_liklihood):\n","        result_class =   \"river\"\n","    elif (max_class == class3_liklihood):\n","        result_class = \"forest\"   \n","\n","    return   result_class\n","\n","\n","print(Classify_Based_on_Maximum_Likelihood(test_forest[0],residual_mean ,residula_covariance,river_mean,river_covariance,forest_mean ,forest_covariance ,p_residual,p_river,p_forest))\n","\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["forest\n"]}],"source":["\n","print(Classify_Based_on_Maximum_Likelihood(test_forest[5],residual_mean ,residula_covariance,river_mean,river_covariance,forest_mean ,forest_covariance ,p_residual,p_river,p_forest))\n"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["y_true =[]\n","y_predicted = []\n","\n","for image in test_residual:\n","    y_predicted.append(Classify_Based_on_Maximum_Likelihood(image,residual_mean ,residula_covariance,river_mean,river_covariance,forest_mean ,forest_covariance ,p_residual,p_river,p_forest))\n","    y_true.append(\"residual\")\n","    \n","for image in test_river:\n","    y_predicted.append(Classify_Based_on_Maximum_Likelihood(image,residual_mean ,residula_covariance,river_mean,river_covariance,forest_mean ,forest_covariance ,p_residual,p_river,p_forest))\n","    y_true.append(\"river\")   \n","    \n","    \n","for image in test_forest:\n","    y_predicted.append(Classify_Based_on_Maximum_Likelihood(image,residual_mean ,residula_covariance,river_mean,river_covariance,forest_mean ,forest_covariance ,p_residual,p_river,p_forest))\n","    y_true.append(\"forest\")       \n","  "]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest']\n"]}],"source":["print(y_true)\n"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['forest', 'residual', 'residual', 'residual', 'forest', 'residual', 'river', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'river', 'river', 'residual', 'river', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'river', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'forest', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'forest', 'forest', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'forest', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'river', 'river', 'residual', 'river', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'forest', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'forest', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'forest', 'river', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'forest', 'residual', 'residual', 'residual', 'forest', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'residual', 'residual', 'residual', 'river', 'river', 'river', 'river', 'residual', 'river', 'forest', 'residual', 'forest', 'river', 'residual', 'river', 'forest', 'river', 'river', 'residual', 'river', 'river', 'residual', 'forest', 'residual', 'forest', 'residual', 'river', 'forest', 'residual', 'river', 'forest', 'residual', 'residual', 'river', 'residual', 'river', 'river', 'river', 'river', 'residual', 'river', 'river', 'river', 'river', 'river', 'river', 'forest', 'river', 'river', 'river', 'river', 'residual', 'river', 'river', 'river', 'forest', 'river', 'river', 'residual', 'residual', 'river', 'river', 'river', 'forest', 'forest', 'river', 'residual', 'river', 'river', 'forest', 'river', 'forest', 'forest', 'river', 'river', 'residual', 'residual', 'river', 'river', 'river', 'forest', 'river', 'river', 'forest', 'river', 'residual', 'residual', 'river', 'forest', 'river', 'river', 'residual', 'residual', 'river', 'residual', 'river', 'river', 'river', 'river', 'river', 'residual', 'forest', 'river', 'river', 'river', 'residual', 'river', 'river', 'river', 'river', 'river', 'forest', 'forest', 'forest', 'river', 'river', 'forest', 'river', 'residual', 'river', 'river', 'river', 'forest', 'river', 'forest', 'residual', 'river', 'river', 'residual', 'residual', 'forest', 'residual', 'river', 'residual', 'river', 'residual', 'river', 'river', 'river', 'river', 'river', 'residual', 'river', 'residual', 'residual', 'residual', 'river', 'river', 'residual', 'river', 'forest', 'residual', 'river', 'river', 'river', 'residual', 'river', 'river', 'residual', 'river', 'river', 'residual', 'river', 'river', 'river', 'river', 'residual', 'river', 'river', 'residual', 'residual', 'residual', 'residual', 'river', 'residual', 'river', 'river', 'residual', 'river', 'river', 'river', 'residual', 'residual', 'river', 'river', 'river', 'river', 'residual', 'river', 'river', 'river', 'river', 'river', 'river', 'forest', 'river', 'forest', 'river', 'river', 'river', 'residual', 'river', 'river', 'river', 'river', 'river', 'residual', 'residual', 'residual', 'river', 'forest', 'river', 'river', 'river', 'river', 'river', 'residual', 'river', 'river', 'residual', 'forest', 'river', 'residual', 'residual', 'forest', 'river', 'river', 'river', 'river', 'forest', 'residual', 'residual', 'forest', 'river', 'residual', 'river', 'residual', 'forest', 'river', 'residual', 'residual', 'river', 'river', 'river', 'river', 'residual', 'river', 'river', 'river', 'residual', 'river', 'river', 'river', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'residual', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'residual', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'river', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest']\n"]}],"source":["print(y_predicted)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation and Understanding "]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Confusion Matrix': array([[244,  45,  11],\n","       [ 67, 148,  35],\n","       [ 18,   8, 274]], dtype=int64), 'Accuracy': 0.7835294117647059, 'Average F1 Score': 0.7720083054650556, 'F1 Scores': {'residual': 0.7758346581875993, 'river': 0.656319290465632, 'forest': 0.8838709677419355}}\n"]}],"source":["\n","labels = [\"residual\",\"river\",\"forest\"]\n","def model_evaluation(y_true,y_predicted,labels):\n","    \n","    cm = confusion_matrix(y_true, y_predicted, labels=labels)\n","    \n","    \n","    accuracy = accuracy_score(y_true, y_predicted)\n","    \n"," \n","    f1_scores = f1_score(y_true, y_predicted, labels=labels, average=None)\n","    \n","\n","    avg_f1_score = np.mean(f1_scores)\n","    \n","   \n","    results = {\n","        'Confusion Matrix': cm,\n","        'Accuracy': accuracy,\n","        'Average F1 Score': avg_f1_score,\n","        'F1 Scores': dict(zip(labels, f1_scores))\n","    }\n","    \n","    return results\n","\n","\n","model_eval = model_evaluation(y_true,y_predicted,labels)\n","print(model_eval)\n","    \n","   "]},{"cell_type":"markdown","metadata":{},"source":["## Submission Instructions\n","\n","To ensure a smooth evaluation process, please follow these steps for submitting your work:\n","\n","1. **Prepare Your Submission:** Alongside your main notebook, include any additional files that are necessary for running the notebook successfully. This might include data files, images, or supplementary scripts.\n","\n","2. **Rename Your Files:** Before submission, please rename your notebook to reflect the IDs of the two students working on this project. The format should be `ID1_ID2`, where `ID1` and `ID2` are the student IDs. For example, if the student IDs are `9123456` and `9876543`, then your notebook should be named `9123456_9876543.ipynb`.\n","\n","3. **Check for Completeness:** Ensure that all required tasks are completed and that the notebook runs from start to finish without errors. This step is crucial for a smooth evaluation.\n","\n","4. **Submit Your Work:** Once everything is in order, submit your notebook and any additional files via the designated submission link on Google Classroom **(code: 2yj6e24)**. Make sure you meet the submission deadline to avoid any late penalties.\n","5. Please, note that the same student should submit the assignments for the pair throughout the semester.\n","\n","By following these instructions carefully, you help us in evaluating your work efficiently and fairly **and any failure to adhere to these guidelines can affect your grades**. If you encounter any difficulties or have questions about the submission process, please reach out as soon as possible.\n","\n","We look forward to seeing your completed projects and wish you the best of luck!\n"]},{"cell_type":"markdown","metadata":{},"source":["# Report Accuracy and Average F1 Score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Grading Rubric (Total: 10 Marks)\n","\n","The lab is graded based on the following criteria:\n","\n","1. **Data Loading and Preparation (2 Marks)**\n","   - Correctly loads images for the residential, river, and forest classes. (0.5 Marks)\n","   - Accurately splits the dataset into training and testing subsets and clearly shows this split. (1.5 Marks)\n","\n","2. **Feature Extraction (2 Marks)**\n","   - Implements feature extraction appropriately, considering the curse of dimensionality. (1 Mark)\n","   - Extracts and justifies the selection of features relevant to distinguishing the classes. (1 Mark)\n","\n","3. **Implementation of MLE Classifier (3 Marks)**\n","   - Correctly calculates and clearly shows class priors and class-specific parameters. (1 Mark)\n","   - Accurately computes likelihoods using the likelihood equation (probability density function) and classifies based on maximum likelihood. Must clearly show these calculations and explain the choice of likelihood equation. (2 Marks)\n","\n","4. **Model Evaluation and Understanding (3 Marks)**\n","   - Shows **confusion matrix** and correctly calculates and clearly shows the calculations for Accuracy and Average F1 Score. (1 Mark)\n","   - **Comparison amongst your peers.** Compares the model's performance against those of peers to identify strengths and areas for improvement. (2 Marks)\n","\n","Each section of the lab will be evaluated on completeness, and correctness in approach and analysis. Part of the rubric also includes the student's ability to explain and justify their choices and results.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def compute_covariance_matrix(images):\n","    \"\"\"\n","    Compute the covariance matrix of a set of multispectral images.\n","\n","    Args:\n","    - images: A 4D NumPy array representing the multispectral images.\n","              Shape: (num_images, height, width, num_bands)\n","\n","    Returns:\n","    - covariance_matrix: The covariance matrix of the images.\n","                         Shape: (num_bands, num_bands)\n","    \"\"\"\n","    # Reshape the images into a 2D array where each row represents a pixel\n","    # and each column represents a spectral band across all images\n","    flattened_images = images.reshape(-1, images.shape[-1])\n","\n","    # Compute the mean vector\n","    mean_vector = np.mean(flattened_images, axis=0)\n","\n","    # Compute the deviation matrix\n","    deviation_matrix = flattened_images - mean_vector\n","\n","    # Compute the covariance matrix\n","    covariance_matrix = np.dot(deviation_matrix.T, deviation_matrix) / flattened_images.shape[0]\n","\n","    return covariance_matrix\n","\n","print(compute_covariance_matrix(train_forest))\n","\n","# Example usage:\n","# Assume 'images' is a 4D NumPy array representing multispectral images\n","# Shape: (num_images, height, width, num_bands)\n","\n","# covariance_matrix = compute_covariance_matrix(images)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
